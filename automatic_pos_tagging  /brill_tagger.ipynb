{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmxe+ylR0xOTuyuv0w/lij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdgarID/foteFatal.py/blob/toyVersion/automatic_pos_tagging%20%20/brill_tagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install nltk & conllu\n",
        "! pip install nltk conllu"
      ],
      "metadata": {
        "id": "OBbgn95xQ6uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import nltk\n",
        "from nltk.tag import brill, brill_trainer\n",
        "from nltk.tag import RegexpTagger\n",
        "from conllu import parse\n",
        "import requests"
      ],
      "metadata": {
        "id": "Sdi0J2AUQ6zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 1. Load and parse UD French corpus\n",
        "# ---------------------------------------------------------------------\n",
        "url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/master/fr_gsd-ud-train.conllu\"\n",
        "data = requests.get(url).text\n",
        "sentences = parse(data)\n",
        "\n",
        "# Extract (word, pos) tuples\n",
        "tagged_sents = [[(t[\"form\"], t[\"upos\"]) for t in s] for s in sentences]\n",
        "\n",
        "# Split train/test\n",
        "train_sents = tagged_sents[:8000]\n",
        "test_sents = tagged_sents[8000:]"
      ],
      "metadata": {
        "id": "gWsG17OLSyuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import UnigramTagger, RegexpTagger, DefaultTagger\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2. Create baseline tagger — Unigram with backoff\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# Step 1: Default tagger (fallback for completely unknown words)\n",
        "default_tagger = DefaultTagger('NOUN')\n",
        "\n",
        "# Step 2: Regex tagger for simple morphological patterns\n",
        "regexp_tagger = RegexpTagger([\n",
        "    (r'.*ment$', 'NOUN'),    # typical noun suffix\n",
        "    (r'.*tion$', 'NOUN'),    # typical noun suffix\n",
        "    (r'.*able$', 'ADJ'),     # typical adjective suffix\n",
        "    (r'.*', 'NOUN')          # catch-all fallback\n",
        "], backoff=default_tagger)\n",
        "\n",
        "# Step 3: Unigram tagger trained on data, backed off by regex/default\n",
        "unigram_tagger = UnigramTagger(train_sents, backoff=regexp_tagger)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Evaluate baseline accuracy\n",
        "# ---------------------------------------------------------------------\n",
        "accuracy = unigram_tagger.evaluate(test_sents)\n",
        "print(\"Unigram baseline accuracy with backoff:\", accuracy)"
      ],
      "metadata": {
        "id": "EdsNW3-DatuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 3. Train Brill tagger on top of the Unigram baseline\n",
        "# ---------------------------------------------------------------------\n",
        "templates = brill.fntbl37()  # 37 default templates from NLTK\n",
        "trainer = brill_trainer.BrillTaggerTrainer(unigram_tagger, templates)\n",
        "brill_tagger = trainer.train(train_sents, max_rules=200)\n",
        "\n",
        "print(\"✅ Brill tagger trained.\")\n",
        "print(\"Accuracy:\", brill_tagger.evaluate(test_sents))"
      ],
      "metadata": {
        "id": "QRYP0hytatw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# ---------------------------------------------------------------------\n",
        "# 4. Save the model\n",
        "# ---------------------------------------------------------------------\n",
        "with open(\"brill_french_tagger.pkl\", \"wb\") as f:\n",
        "    pickle.dump(brill_tagger, f)\n",
        "\n",
        "print(\"✅ Saved model to brill_french_tagger.pkl\")"
      ],
      "metadata": {
        "id": "m8EnQhAGat0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download punktab\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "-2NcJ1Kkbe8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pickle\n",
        "import csv\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1. Load the trained Brill tagger\n",
        "# ---------------------------------------------------------------------\n",
        "with open(\"brill_french_tagger.pkl\", \"rb\") as f:\n",
        "    brill_tagger = pickle.load(f)\n",
        "\n",
        "print(\"✅ Brill tagger loaded.\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2. Read the new French text file\n",
        "# ---------------------------------------------------------------------\n",
        "with open(\"./Apprenants polonophones du FLE_Licence 2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text, language=\"french\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3. Apply the Brill tagger\n",
        "# ---------------------------------------------------------------------\n",
        "tagged = brill_tagger.tag(tokens)\n",
        "\n",
        "# Print first 20 tagged tokens\n",
        "for word, tag in tagged[:20]:\n",
        "    print(f\"{word:<20} {tag}\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4. Save results to CSV\n",
        "# ---------------------------------------------------------------------\n",
        "with open(\"brill_pos_output.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"TOKEN\", \"POS\"])\n",
        "    for word, tag in tagged:\n",
        "        writer.writerow([word, tag])\n",
        "\n",
        "print(\"✅ POS tagging results saved to brill_pos_output.csv\")\n"
      ],
      "metadata": {
        "id": "kvejvCttat2w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}